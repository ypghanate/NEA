import os
import warnings
import torch
from music21 import converter
from django.conf import settings
from basic_pitch.inference import predict_and_save
from basic_pitch import ICASSP_2022_MODEL_PATH

from .model import SeqSimplifier
from .utils import midi_to_pianoroll, pianoroll_to_midi, add_difficulty_channel, overallComplexity

warnings.filterwarnings('ignore', category=UserWarning, module='resampy')
os.environ["BASIC_PITCH_BACKEND"] = "CoreML"

MODEL_PATH = os.path.join(settings.BASE_DIR, "simplifier_model.pth")
simplifier_model = SeqSimplifier()
if os.path.exists(MODEL_PATH):
    simplifier_model.load_state_dict(torch.load(MODEL_PATH, map_location='cpu'))
simplifier_model.eval()  

def audio_to_midi(audio_path, output_midi_path):
    if not os.path.exists(audio_path):
        raise FileNotFoundError(f"Audio file not found: {audio_path}")

    os.makedirs(os.path.dirname(output_midi_path), exist_ok=True)

    predict_and_save(
        [audio_path],
        os.path.dirname(output_midi_path),
        save_midi=True,
        sonify_midi=False,
        save_model_outputs=False,
        save_notes=False,
        model_or_model_path=str(ICASSP_2022_MODEL_PATH)
    )

    generated_files = [f for f in os.listdir(os.path.dirname(output_midi_path)) if f.endswith('.mid')]
    if not generated_files:
        raise FileNotFoundError("MIDI file not generated by Basic Pitch")

    os.rename(
        os.path.join(os.path.dirname(output_midi_path), generated_files[0]),
        output_midi_path
    )


def convert_audio_sheet(midi_path, user_skill):
    if not os.path.exists(midi_path):
        raise FileNotFoundError(f"MIDI file not found: {midi_path}")

    roll = midi_to_pianoroll(midi_path)
    sheet_diff = overallComplexity(midi_path, roll)

    if sheet_diff == 'hard' and user_skill in ['easy', 'medium']:
        target = 'easy'
    elif sheet_diff == 'easy' and user_skill == 'hard':
        target = 'hard'
    else:
        target = sheet_diff

    roll_input = add_difficulty_channel(roll, target)
    roll_tensor = torch.tensor(roll_input[None, :, :]).float() 

    with torch.no_grad():
        output_roll = simplifier_model(roll_tensor).squeeze(0).numpy()

    output_roll = (output_roll > 0.5).astype(float)

    base = os.path.splitext(os.path.basename(midi_path))[0]
    sheet_dir = os.path.join(settings.MEDIA_ROOT, 'uploads', 'sheet_music')
    os.makedirs(sheet_dir, exist_ok=True)

    midi_out = os.path.join(sheet_dir, f"{base}_{target}.mid")
    xml_out = os.path.join(sheet_dir, f"{base}_{target}.xml")

    pianoroll_to_midi(output_roll, midi_out)
    quantize_midi(midi_out, resolution=0.25)
    score = converter.parse(midi_out)
    score.write('musicxml', fp=xml_out)

    return {
        "midi": midi_out,
        "xml": xml_out,
        "target_difficulty": target
    }


def quantize_midi(midi_file_path, resolution=0.25):
    from pretty_midi import PrettyMIDI

    pm = PrettyMIDI(midi_file_path)
    for instrument in pm.instruments:
        for note in instrument.notes:
            note.start = round(note.start / resolution) * resolution
            note.end = round(note.end / resolution) * resolution
    pm.write(midi_file_path)

