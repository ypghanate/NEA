# import os
# import warnings
# import torch
# from music21 import converter
# from django.conf import settings
# from basic_pitch.inference import predict_and_save
# from basic_pitch import ICASSP_2022_MODEL_PATH

# from .model import SeqSimplifier
# from .utils import midi_to_pianoroll, pianoroll_to_midi, add_difficulty_channel, overallComplexity

# warnings.filterwarnings('ignore', category=UserWarning, module='resampy')
# os.environ["BASIC_PITCH_BACKEND"] = "CoreML"

# MODEL_PATH = os.path.join(settings.BASE_DIR, "simplifier_model.pth")
# simplifier_model = SeqSimplifier()
# if os.path.exists(MODEL_PATH):
#     simplifier_model.load_state_dict(torch.load(MODEL_PATH, map_location='cpu'))
# simplifier_model.eval()  

# def audio_to_midi(audio_path, output_midi_path):
#     if not os.path.exists(audio_path):
#         raise FileNotFoundError(f"Audio file not found: {audio_path}")

#     os.makedirs(os.path.dirname(output_midi_path), exist_ok=True)

#     predict_and_save(
#         [audio_path],
#         os.path.dirname(output_midi_path),
#         save_midi=True,
#         sonify_midi=False,
#         save_model_outputs=False,
#         save_notes=False,
#         model_or_model_path=str(ICASSP_2022_MODEL_PATH)
#     )

#     # Rename first generated MIDI to match output path
#     generated_files = [f for f in os.listdir(os.path.dirname(output_midi_path)) if f.endswith('.mid')]
#     if not generated_files:
#         raise FileNotFoundError("MIDI file not generated by Basic Pitch")

#     os.rename(
#         os.path.join(os.path.dirname(output_midi_path), generated_files[0]),
#         output_midi_path
#     )


# def convert_audio_sheet(midi_path, user_skill):
#     if not os.path.exists(midi_path):
#         raise FileNotFoundError(f"MIDI file not found: {midi_path}")

#     roll = midi_to_pianoroll(midi_path)
#     sheet_diff = overallComplexity(midi_path, roll)

#     # Determine target difficulty based on user skill
#     if sheet_diff == 'hard' and user_skill in ['easy', 'medium']:
#         target = 'easy'
#     elif sheet_diff == 'easy' and user_skill == 'hard':
#         target = 'hard'
#     else:
#         target = sheet_diff

#     roll_input = add_difficulty_channel(roll, target)
#     roll_tensor = torch.tensor(roll_input[None, :, :]).float()  # batch dimension

#     # Inference using preloaded model
#     with torch.no_grad():
#         output_roll = simplifier_model(roll_tensor).squeeze(0).numpy()

#     # Threshold output to binary piano roll
#     output_roll = (output_roll > 0.5).astype(float)

#     # Prepare output paths
#     base = os.path.splitext(os.path.basename(midi_path))[0]
#     sheet_dir = os.path.join(settings.MEDIA_ROOT, 'uploads', 'sheet_music')
#     os.makedirs(sheet_dir, exist_ok=True)

#     midi_out = os.path.join(sheet_dir, f"{base}_{target}.mid")
#     xml_out = os.path.join(sheet_dir, f"{base}_{target}.xml")

#     # Convert piano roll to MIDI, quantize, then export MusicXML
#     pianoroll_to_midi(output_roll, midi_out)
#     quantize_midi(midi_out, resolution=0.25)
#     score = converter.parse(midi_out)
#     score.write('musicxml', fp=xml_out)

#     return {
#         "midi": midi_out,
#         "xml": xml_out,
#         "target_difficulty": target
#     }

# def quantize_midi(midi_file_path, resolution=0.25):
#     """
#     Quantize note start times and durations to nearest `resolution` beats.
#     """
#     from pretty_midi import PrettyMIDI

#     pm = PrettyMIDI(midi_file_path)
#     for instrument in pm.instruments:
#         for note in instrument.notes:
#             note.start = round(note.start / resolution) * resolution
#             note.end = round(note.end / resolution) * resolution
#     pm.write(midi_file_path)

import sys, os
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
import warnings
import torch
from music21 import converter
from django.conf import settings
from basic_pitch.inference import predict_and_save
from basic_pitch import ICASSP_2022_MODEL_PATH
os.environ['DJANGO_SETTINGS_MODULE'] = 'myproject.settings'
import django
django.setup()


warnings.filterwarnings('ignore', category=UserWarning, module='resampy')
os.environ["BASIC_PITCH_BACKEND"] = "CoreML"
# audio_path = "/Users/sudhirghanate/Downloads/sampleaudio.mp3"


def audio_to_midi(audio_path, output_midi_path):
    if not os.path.exists(audio_path):
        raise FileNotFoundError(f"Audio file not found: {audio_path}")

    os.makedirs(os.path.dirname(output_midi_path), exist_ok=True)

    if os.path.exists(output_midi_path):
        os.remove(output_midi_path)

    predict_and_save(
        [audio_path],
        os.path.dirname(output_midi_path),
        save_midi=True,
        sonify_midi=False,
        save_model_outputs=False,
        save_notes=False,
        model_or_model_path=str(ICASSP_2022_MODEL_PATH)
    )

    generated_files = [f for f in os.listdir(os.path.dirname(output_midi_path)) if f.endswith('.mid')]
    if not generated_files:
        raise FileNotFoundError("MIDI file not generated by Basic Pitch")

    os.rename(
        os.path.join(os.path.dirname(output_midi_path), generated_files[0]),
        output_midi_path
    )


# audio_to_midi("/Users/sudhirghanate/Downloads/sampleaudio.mp3", '~/Downloads/sampleaudio.mid')

def convert_audio_sheet(output_midi_path):
    if not os.path.exists(output_midi_path):
        raise FileNotFoundError(f"MIDI file not found: {output_midi_path}")
    
    base = os.path.splitext(os.path.basename(output_midi_path))[0]
    sheet_dir = os.path.join(settings.MEDIA_ROOT, 'uploads', 'sheet_music')
    os.makedirs(sheet_dir, exist_ok=True)

    midi_out = os.path.join(sheet_dir, f"{base}.mid")
    xml_out = os.path.join(sheet_dir, f"{base}.musicxml")
    
    score = converter.parse(output_midi_path)
    score.write('musicxml', fp=xml_out)

    return {
        "midi": midi_out,
        "xml": xml_out,
    }
# print(convert_audio_sheet('~/Downloads/sampleaudio.mid'))

